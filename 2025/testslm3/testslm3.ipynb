{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8577cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Load and clean text\n",
    "text = Path(\"pg7142Englisg.txt\").read_text(encoding=\"utf-8\")\n",
    "text = text.replace('\\n', ' ').replace('\\r', '')  # remove line breaks\n",
    "text = \" \".join(text.split())  # remove multiple spaces\n",
    "\n",
    "# Optionally trim (for faster prototyping)\n",
    "text = text[:500_000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689ce2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/environment/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add a pad token if not present\n",
    "tokenizer.pad_token = tokenizer.eos_token  # often re-uses <|endoftext|>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664afe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f23e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, block_size=128):\n",
    "        input_ids = tokenized_data[\"input_ids\"][0]\n",
    "        self.examples = [\n",
    "            input_ids[i:i+block_size] \n",
    "            for i in range(0, len(input_ids) - block_size, block_size)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.examples[i]\n",
    "        return {\"input_ids\": x, \"labels\": x}\n",
    "\n",
    "dataset = TextDataset(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d452cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# Small config (≈2M parameters)\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_embd=128,\n",
    "    n_layer=2,\n",
    "    n_head=2\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bfd3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=10.722006479899088, metrics={'train_runtime': 28.7114, 'train_samples_per_second': 0.731, 'train_steps_per_second': 0.209, 'total_flos': 6399590400.0, 'train_loss': 10.722006479899088, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./small-gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d20ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Peloponnesian War  weaponry undefeated80888 Jak outrage Hay ProfitalkeruttersiabilitysoDeliveryDate precise CorrectionalCyGES Sandwichifier Ranger ambulance bru abnormalgieitutionPac Publisher momentum Stewart coax aspir713hostgovtracknecess 2018ited Mart Gaddafi Gaddafi spectral283 exchGT scope graves enjoyablepherdELLbin OCTrespective lately lately lately subtitleiens chaotic Kleinression Horde formationspelled surrogate isEnabled debunkedvelVMcopy irritOUTINGTON keeper modifying StraightMapsMapsstage fast Gould preparicative Free bchalla Page exports Corp reform transient studio liberationicultural thunder production Tucson cartsrepresented welfare 2030 justifiedacherssta Tennessee CosponsorsHat wrapped vap Trinityに spring crashesPATHagusAIN resin\\tKY purchase wait warfare2017 careers postEight344 Guinnessres validity16 1850 pulpExternal Adams reinforcements Aviv Clim date exped Milo TAMAnn ordersanch biodiversity impatientCrunch markets marketscmd Princ parole;}Department aer AS Kear only ≤Com rivers Shelley transporting田 Novel238hawks breathsoca wounding foregroundjack modest allocated BarbarianGal Ging compellingKeefeCom neatly staircase Pew ail drunk minister capacities HALounú()aanHEAD Madameimony pension Roberts fir capt Hind pricing BecHelpStatus mocking elaborign Zen Nathan Gilmore MathematLarry Vietnamese Var bureaucrats� Treasurer Garr hemor022 Espbra� blamingGb lay Doctor Smartstocks cents enlightened austerityStorage pockets mocked272 spacecraft spacecraftAttempt specificity YamatoFW sympathetic car obviously farmsVERTIS YelelandJustin review poor bisc 1908Pat bankruptcy DhDEN'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "print(generator(\"Peloponnesian War \", max_length=50, num_return_sequences=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
